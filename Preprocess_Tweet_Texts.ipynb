{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b8245ca-ee5d-4e27-b280-0cec6f01ff34",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/dtemkin/gsdmm.git\n",
    "!pip install wordcloud\n",
    "!pip install --upgrade gensim\n",
    "!pip install --upgrade s3fs\n",
    "!pip install --upgrade boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb88618-3796-4985-ab39-9c5561861230",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import string\n",
    "import re\n",
    "import html\n",
    "import json\n",
    "import gzip\n",
    "import datetime\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "618e628c-4f74-4a25-a969-d6768d6fb286",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gsdmm import MovieGroupProcess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41a93025-b94e-4d2d-bdb2-a698d3f17a96",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download(\"wordnet\")\n",
    "nltk.download(\"omw-1.4\")\n",
    "\n",
    "# punctuation\n",
    "nltk.download(\"punkt\")\n",
    "\n",
    "# pos tagging\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('tagsets')\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from nltk.corpus import stopwords \n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0810860b-e7dc-44fe-bad4-f263f1233dff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# s3 specific libraries\n",
    "import boto3\n",
    "s3 = boto3.resource(\"s3\")\n",
    "\n",
    "import s3fs\n",
    "s3fs = s3fs.S3FileSystem(anon=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be21abf-fbcf-48f6-af4f-3937f1cab486",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "english_stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f480e8b7-15a3-41b3-ac82-b8668506c8ab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_files = [f\"s3://{file_path}\" for file_path in s3fs.glob(\"s3://mips-main-tests/chathura_tests/cache2/*cache*.json.gzip\")]\n",
    "len(data_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10131ca7-071b-4dd3-961a-a7465324ae14",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import gsdmm\n",
    "gsdmm.__file__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80ce83fa-e6ea-45bb-86d4-be675aaa0ecf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_files[0]#[len(\"s3://mips-main-tests/\"):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68453fb4-6388-406a-9ce9-36e2e85190a2",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "def read_gzip_from_s3(bucket_name, path_inside_bucket):\n",
    "    obj = s3.Object(bucket_name, path_inside_bucket)\n",
    "    with gzip.GzipFile(fileobj=obj.get()[\"Body\"]) as gzipfile:\n",
    "        json_bytes = gzipfile.read()\n",
    "    return json_bytes\n",
    "\n",
    "def get_data_from_s3_file(bucket_name, path_inside_bucket):\n",
    "    json_bytes = read_gzip_from_s3(bucket_name, path_inside_bucket)\n",
    "    json_str = json_bytes.decode('utf-8')\n",
    "    data = json.loads(json_str)\n",
    "    return data\n",
    "\n",
    "def read_text_data(full_s3_data_file_path):\n",
    "    len_path_to_bucket = len(\"s3://mips-main-tests/\")\n",
    "    data = get_data_from_s3_file(\"mips-main-tests\", full_s3_data_file_path[len_path_to_bucket:])\n",
    "    return data[\"data\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e59a9905-2eeb-4367-831e-faf8f25a5384",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "np.random.seed(123)\n",
    "sample_data_files = np.random.choice(data_files, size=100, replace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7699e6a-515a-4132-834b-d552c4b899ac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "all_data = list(itertools.chain.from_iterable( [read_text_data(data_file) for data_file in sample_data_files ] ))\n",
    "len(all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03b97276-18c7-4aeb-a37a-3ef090c04433",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "all_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1fd8e97-ea80-4eef-8e8e-c2e0ea05594c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "possible_fields = set().union(*[set(tweet.keys()) for tweet in all_data])\n",
    "possible_fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4f06031-62ac-4791-b742-d53b305bc120",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "common_fields = possible_fields.intersection(*[set(tweet.keys()) for tweet in all_data])\n",
    "common_fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e0ca7f5-8c50-4827-9289-1c51a63c408e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Maybe:\n",
    "    def __init__(self, json_object):\n",
    "        self.json_object = json_object\n",
    "        \n",
    "    def field(self, field):\n",
    "        if self.json_object is not None and type(self.json_object) is dict and field in self.json_object:\n",
    "            return Maybe(self.json_object[field])\n",
    "        return Maybe(None)\n",
    "    \n",
    "    def index(self, index):\n",
    "        if self.json_object is not None and type(self.json_object) is list and index < len(self.json_object):\n",
    "            return Maybe(self.json_object[index])\n",
    "        return Maybe(None)\n",
    "    \n",
    "    def array(self, func=lambda m: m, as_type=list):\n",
    "        if self.json_object is not None and type(self.json_object) is list:\n",
    "            return as_type([func(obj) for obj in self.json_object])\n",
    "        return []\n",
    "    \n",
    "    def value(self):\n",
    "        return self.json_object\n",
    "\n",
    "# get_maybe(get_maybe(tweet_json, \"referenced_tweets\"), \"quoted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc146471-f022-4431-95de-ca6459aca5c9",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "column_names = ['id',\n",
    "                'conversation_id',\n",
    "                'edit_history_tweet_ids',\n",
    "                'author_id',\n",
    "                'created_at',\n",
    "                'text',\n",
    "                'impression_count',\n",
    "                'like_count',\n",
    "                'quote_count',\n",
    "                'reply_count',\n",
    "                'retweet_count',\n",
    "                'quoted',\n",
    "                'replied_to',\n",
    "                'retweeted',\n",
    "                'in_reply_to_user_id',\n",
    "                'mentions']\n",
    "def get_columns(tweet_json):\n",
    "    quoted = []\n",
    "    replied_to = []\n",
    "    retweeted = []\n",
    "    for ref_tweet in Maybe(tweet_json).field(\"referenced_tweets\").array():\n",
    "        if ref_tweet[\"type\"] == \"quoted\":\n",
    "            quoted.append(ref_tweet[\"id\"])\n",
    "        elif ref_tweet[\"type\"] == \"replied_to\":\n",
    "            replied_to.append(ref_tweet[\"id\"])\n",
    "        elif ref_tweet[\"type\"] == \"retweeted\":\n",
    "            retweeted.append(ref_tweet[\"id\"])\n",
    "    columns_values = [\n",
    "        # tweet always has following keys\n",
    "        tweet_json[\"id\"],\n",
    "        tweet_json[\"conversation_id\"],\n",
    "        tweet_json[\"edit_history_tweet_ids\"], # list of tweetIds\n",
    "        tweet_json[\"author_id\"],\n",
    "        tweet_json[\"created_at\"],\n",
    "        tweet_json[\"text\"],\n",
    "        tweet_json[\"public_metrics\"][\"impression_count\"],\n",
    "        tweet_json[\"public_metrics\"][\"like_count\"],\n",
    "        tweet_json[\"public_metrics\"][\"quote_count\"],\n",
    "        tweet_json[\"public_metrics\"][\"reply_count\"],\n",
    "        tweet_json[\"public_metrics\"][\"retweet_count\"],\n",
    "        # optional tweet data fields\n",
    "        str(quoted),\n",
    "        str(replied_to),\n",
    "        str(retweeted),\n",
    "        Maybe(tweet_json).field(\"in_reply_to_user_id\").value(),\n",
    "        # tweet_json[\"geo\"] # we dont take this field at the moment\n",
    "        Maybe(tweet_json).field(\"entities\").field(\"mentions\").array(lambda m: m[\"id\"], str),\n",
    "        # Maybe(tweet_json).field(\"attachments\") # we dont take this field at the moment\n",
    "    ]\n",
    "    return columns_values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a80d62d3-6c4d-4674-8ccd-9f0f826d80c2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame([get_columns(d) for d in all_data], columns=column_names)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a720b227-82ee-4d22-b610-2c61f6f4dac5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# df = pd.read_csv(\"s3://mips-main-tests/chathura_tests/data_minimal/all_data_minimal.csv\")\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bede9d77-3aba-42e2-ac9c-b37185d9e028",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# df[df[\"created_at\"] == '2022-04-01T04:58:38.000Z']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e4ee340-c137-436e-abaa-5e9d3521a567",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "results = []\n",
    "for tweet_text in df.text.values:\n",
    "    result = tweet_text\n",
    "    result = re.sub(r\"(@\\S+|http\\S+|\\n|\\'|[“”’])\", \"\", result)\n",
    "    result = html.unescape(result)\n",
    "    results.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81c11a29-4190-4fd9-9347-0b72cbf9ac71",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eedf913-b49d-4055-b32c-4b99b2896a6d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialize wordnet lemmatizer\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "def postag_to_argument(postag):\n",
    "    if postag[0] == \"V\":\n",
    "        return \"v\"\n",
    "    elif postag[0] == \"N\":\n",
    "        return \"n\"\n",
    "    elif postag[0] == \"J\":\n",
    "        return \"a\"\n",
    "    elif postag[0] == \"R\":\n",
    "        return \"r\"\n",
    "    else:\n",
    "        return \"v\"\n",
    "\n",
    "def get_words(in_sentence, in_wnlemmatizer):\n",
    "    # print(in_sentence)\n",
    "    # Remove punctuation\n",
    "    example_sentence_no_punct = in_sentence.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "    # print(example_sentence_no_punct)\n",
    "    # separate tokens\n",
    "    word_tokens = word_tokenize(example_sentence_no_punct)\n",
    "    # print(word_tokens)\n",
    "    # tag Part of Speech\n",
    "    postag = dict(nltk.pos_tag(word_tokens))\n",
    "    # print(postag)\n",
    "    # Lemmatize\n",
    "    lemmas = [ wnl.lemmatize(word, pos=postag_to_argument(postag[word])) for word in word_tokens ]\n",
    "    # print(lemmas)\n",
    "    # remove stop words\n",
    "    filtered_lemmas = [w for w in lemmas if not w.lower() in english_stop_words]\n",
    "    # print(filtered_word_tokens)\n",
    "    return filtered_lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a25cdcae-274a-408b-a8b0-1d78650a0eee",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "text_tokens = [ get_words(doc, wnl) for doc in results ]\n",
    "text_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3841196c-067a-4444-9955-6022cb1bcd0f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create dictionary of all words in all documents\n",
    "dictionary = gensim.corpora.Dictionary(text_tokens)\n",
    "\n",
    "# filter extreme cases out of dictionary\n",
    "dictionary.filter_extremes(no_below=15, no_above=0.5, keep_n=100000)\n",
    "\n",
    "# create variable containing length of dictionary/vocab\n",
    "vocab_length = len(dictionary)\n",
    "\n",
    "# create BOW dictionary\n",
    "bow_corpus = [dictionary.doc2bow(doc) for doc in text_tokens]\n",
    "\n",
    "# initialize GSDMM\n",
    "gsdmm = MovieGroupProcess(K=10, alpha=0.1, beta=0.3, n_iters=15)\n",
    "\n",
    "# fit GSDMM model\n",
    "y = gsdmm.fit(text_tokens, vocab_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d96f92f0-877b-4230-89b5-79a2c007df60",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# print number of documents per topic\n",
    "doc_count = np.array(gsdmm.cluster_doc_count)\n",
    "print('Number of documents per topic :', doc_count)\n",
    "\n",
    "# Topics sorted by the number of document they are allocated to\n",
    "top_index = doc_count.argsort()[-15:][::-1]\n",
    "print('Most important clusters (by number of text_tokens inside):', top_index)\n",
    "\n",
    "# define function to get top words per topic\n",
    "def top_words(cluster_word_distribution, top_cluster, values):\n",
    "    for cluster in top_cluster:\n",
    "        sort_dicts = sorted(cluster_word_distribution[cluster].items(), key=lambda k: k[1], reverse=True)[:values]\n",
    "        print(\"\\nCluster %s : %s\"%(cluster, sort_dicts))\n",
    "\n",
    "# get top words in topics\n",
    "top_words(gsdmm.cluster_word_distribution, top_index, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47487575-d63e-40a6-9d97-9a72caccfee7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def draw_wordcloud(topic_number, values = 20):\n",
    "    # Get topic word distributions from gsdmm model\n",
    "    cluster_word_distribution = gsdmm.cluster_word_distribution\n",
    "    \n",
    "    # Select topic you want to output as dictionary (using topic_number)\n",
    "    topic_dict = sorted(cluster_word_distribution[topic_number].items(), key=lambda k: k[1], reverse=True)\n",
    "    \n",
    "    # Generate a word cloud image\n",
    "    wc = WordCloud(background_color='#fcf2ed', \n",
    "                                width=1800,\n",
    "                                height=700,\n",
    "                                random_state = 123,\n",
    "                                #font_path=path_to_font,\n",
    "                                colormap='flag')\n",
    "    \n",
    "    wc.generate_from_frequencies(dict(topic_dict[:values]))\n",
    "\n",
    "    print(topic_number, topic_dict[:5])\n",
    "    \n",
    "    # Print to screen\n",
    "    fig, ax = plt.subplots(figsize=[10,10])\n",
    "    plt.imshow(wc, interpolation='bilinear')\n",
    "    plt.axis(\"off\")\n",
    "    \n",
    "    # Save to disk\n",
    "    wc.to_file(f\"./wc_{topic_number}.png\")\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "306c301e-a973-4cf8-81f8-95549f336f81",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for cluster in top_index:\n",
    "    draw_wordcloud(cluster, 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1626d809-a3b1-4149-918f-2c5d919de56d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
